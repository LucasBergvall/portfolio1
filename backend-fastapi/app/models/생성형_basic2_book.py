import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from collections import Counter
import re
from torch.nn.utils.rnn import pad_sequence
import torch.nn as nn
import difflib

from sqlalchemy import create_engine
import os




# ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° URL ìˆ˜ì •: ì‚¬ìš©ì, ë¹„ë°€ë²ˆí˜¸, í˜¸ìŠ¤íŠ¸, í¬íŠ¸, DBëª…ì„ ì˜¬ë°”ë¥´ê²Œ ë„£ìœ¼ì„¸ìš”.
DATABASE_URL = "mysql+pymysql://root:1234@192.168.0.95:13306/db1"
engine = create_engine(DATABASE_URL)

# MySQLì˜ book_info í…Œì´ë¸”ì„ DataFrameìœ¼ë¡œ ì½ê¸°
book_df = pd.read_sql("SELECT * FROM book_info", engine)

# CSV íŒŒì¼ê³¼ ë‹¬ë¦¬ ë°ì´í„°ë² ì´ìŠ¤ì˜ ì»¬ëŸ¼ëª…ì´ ë‹¤ë¥¼ ê²½ìš° ê¸°ì¡´ CSVì˜ ì—´ê³¼ ë§¤ì¹­ë˜ë„ë¡ ì´ë¦„ ë³€ê²½
book_df.rename(columns={
    "book_name_info": "íƒ€ì´í‹€", 
    "writer_info": "ì‘ê°€",
    "book_detail_info": "ë‚´ìš©",
    "publisher_info": "ì¶œíŒì‚¬",
    "price_info": "ì›ë³¸ ê°€ê²©",
    "genre_info": "ì¹´í…Œê³ ë¦¬",
    "img_name_info": "ì´ë¯¸ì§€ íŒŒì¼ëª…"
}, inplace=True)


df = pd.read_csv(r"C:\HTMLHak\proj2\p2react\backend\fastapi\app\models\data\bookstore_chatbot_qa1_utf8.csv")
# ì§ˆë¬¸-ë‹µë³€ ê²°í•© ë° ì •ì œ
def clean_text(text):
    return re.sub(r"[^ê°€-í£a-zA-Z0-9\s.,!?~]", "", str(text))


def extract_question_info(question):
    title, author, genre, requested_info = None, None, None, None

    if not any(k in question for k in ["ì œëª©", "ì‘ê°€", "ì†Œì„¤", "ì¥ë¥´", "ë‚´ìš©", "ì¤„ê±°ë¦¬", "ì¹´í…Œê³ ë¦¬"]):
        # ì±… ê´€ë ¨ í‚¤ì›Œë“œ ì—†ìœ¼ë©´ ì•„ì˜ˆ ì‹œë„í•˜ì§€ ì•ŠìŒ
        return None, None, None, None, False
    

    # ì±… ì œëª© í›„ë³´ ì°¾ê¸°
    titles = book_df['íƒ€ì´í‹€'].tolist()
    title_matches = difflib.get_close_matches(question, titles, n=1, cutoff=0.7)
    if title_matches:
        title = title_matches[0]

    # ì‘ê°€ í›„ë³´ ì°¾ê¸°
    authors = book_df['ì‘ê°€'].unique().tolist()
    author_matches = difflib.get_close_matches(question, authors, n=1, cutoff=0.8)
    if author_matches:
        author = author_matches[0]

    # ì¹´í…Œê³ ë¦¬ í›„ë³´ ì°¾ê¸°
    genres = book_df['ì¹´í…Œê³ ë¦¬'].unique().tolist()
    genre_matches = difflib.get_close_matches(question, genres, n=1, cutoff=0.9)
    if genre_matches:
        genre = genre_matches[0]

    # ìš”ì²­ëœ ì •ë³´ íŒë‹¨
    # ì¤‘ë³µ í‚¤ì›Œë“œ ì œê±°
    if any(k in question for k in ["ì‘ê°€", "ì €ì"]):
        requested_info = "ì‘ê°€"
    elif any(k in question for k in ["ì¹´í…Œê³ ë¦¬", "ì¥ë¥´"]):
        requested_info = "ì¹´í…Œê³ ë¦¬"
    elif any(k in question for k in ["ì„¤ëª…", "ë‚´ìš©", "ì¤„ê±°ë¦¬"]):
        requested_info = "ë‚´ìš©"
    else:
        requested_info = None


    has_book_info = any([title, author, genre])
    return title, author, genre, requested_info, has_book_info




def transform_metadata(title, author, genre, description):
    title = title or "ì•Œ ìˆ˜ ì—†ìŒ"
    author = author or "ì•Œ ìˆ˜ ì—†ìŒ"
    genre = genre or "ì•Œ ìˆ˜ ì—†ìŒ"
    return f"ì±… ì œëª©: <TITLE>, ì‘ê°€: <AUTHOR>, ì¹´í…Œê³ ë¦¬: <GENRE>, ì„¤ëª…: <DESC>"\
        .replace("<TITLE>", title).replace("<AUTHOR>", author)\
        .replace("<GENRE>", genre).replace("<DESC>", description)


def remove_particles(sentence):
    # ìì£¼ ì‚¬ìš©ë˜ëŠ” ì¡°ì‚¬ ë¦¬ìŠ¤íŠ¸
    particles = ["ì´", "ê°€", "ì€", "ëŠ”", "ì„", "ë¥¼", "ì—", "ì—ì„œ", "ì—ê²Œ", "ê»˜", "ë„", "ë§Œ", "ê³¼", "ì™€", "ë¡œ", "ìœ¼ë¡œ", "ë¶€í„°", "ê¹Œì§€", "ì˜", "ì—ê²Œì„œ"]
    for p in particles:
        sentence = re.sub(rf"\b{p}\b", "", sentence)  # ë„ì–´ì“°ê¸° ê¸°ì¤€ìœ¼ë¡œ ì¡°ì‚¬ ì œê±°
    return sentence.strip()




# ì±… ì œëª©ì„ ê²€ìƒ‰í•˜ëŠ” í•¨ìˆ˜
def search_book(title):
    result = book_df[book_df["íƒ€ì´í‹€"].str.contains(title, na=False) | book_df["ë‚´ìš©"].str.contains(title, na=False)]
    return result if not result.empty else None

def search_book_info(title=None, author=None, genre=None, info_type=None):
    # ìš°ì„  ì±… ì œëª©ì´ ìˆë‹¤ë©´ ê·¸ê±¸ ê¸°ì¤€ìœ¼ë¡œ ê²€ìƒ‰
    if title is not None:
        row = book_df[book_df['íƒ€ì´í‹€'] == title]
        if not row.empty:
            row = row.iloc[0]
            if info_type == "ì‘ê°€":
                return row['ì‘ê°€']
            elif info_type == "ì¹´í…Œê³ ë¦¬":
                return row['ì¹´í…Œê³ ë¦¬']
            elif info_type == "ë‚´ìš©":
                return row['ë‚´ìš©']

            else:
                # ìš”ì²­ ì •ë³´ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ì ìœ¼ë¡œ ì±… ì œëª© ë°˜í™˜
                return title

    # ì œëª© ì—†ì„ ë•Œ ì‘ê°€ë‚˜ ì¥ë¥´ë¡œ ê²€ìƒ‰ (ê°„ë‹¨íˆ í•´ë‹¹ ì‘ê°€/ì¥ë¥´ ëª©ë¡ ë°˜í™˜)
    if info_type == "ì‘ê°€" and author is not None:
        books = book_df[book_df['ì‘ê°€'] == author]['íƒ€ì´í‹€'].tolist()
        return f"{author} ì‘ê°€ì˜ ì±… ëª©ë¡: {', '.join(books)}" if books else None
    if info_type == "ì¥ë¥´" and genre is not None:
        books = book_df[book_df['ì¹´í…Œê³ ë¦¬'] == genre]['íƒ€ì´í‹€'].tolist()
        return f"{genre} ì¥ë¥´ì˜ ì±… ëª©ë¡: {', '.join(books)}" if books else None

    return None
# ì§ˆë¬¸-ë‹µë³€ ë°ì´í„° ë³€í™˜
qna_sentences = [
    transform_metadata(
        title=row["íƒ€ì´í‹€"] if "ì±… ì œëª©" in row else "ì•Œ ìˆ˜ ì—†ìŒ",
        author=row["ì‘ê°€"] if "ì‘ê°€" in row else "ì•Œ ìˆ˜ ì—†ìŒ",
        genre=row["ì¹´í…Œê³ ë¦¬"] if "ì¹´í…Œê³ ë¦¬" in row else "ì•Œ ìˆ˜ ì—†ìŒ",
        description=f"ì§ˆë¬¸: {remove_particles(clean_text(row['ì§ˆë¬¸']))} ë‹µë³€: {clean_text(row['ë‹µë³€'])}"
    )
    for _, row in df.iterrows()
]

book_sentences = [
    f"<BOOK_INFO_BOOST> {transform_metadata(title, author, genre, desc)}"
    for title, author, genre, desc in zip(book_df["íƒ€ì´í‹€"], book_df["ì‘ê°€"], book_df["ì¹´í…Œê³ ë¦¬"], book_df["ë‚´ìš©"])
]

# ë°ì´í„° í†µí•©
sentences = qna_sentences + book_sentences  # ì¼ë°˜ ì§ˆë¬¸ + ì±… ë°ì´í„° í†µí•©

# ğŸ”¹ `<TITLE>`ì„ ë‹¨ì–´ ì‚¬ì „ì— í™•ì‹¤íˆ í¬í•¨
#book_titles = list(book_data["íƒ€ì´í‹€"])  # ì±… ì œëª© ë¦¬ìŠ¤íŠ¸

def tokenize(sentence):
    return re.findall(r"\w+|[.,!?]", sentence)

tokens = [word for s in sentences for word in s.split()]
vocab = ["<PAD>", "<BOS>", "<EOS>", "<UNK>", "<TITLE>", "<AUTHOR>", "<GENRE>", "<DESC>"] + list(set(tokens))

#vocab.extend(book_titles)  # ì œëª©ì„ vocabì— ì¶”ê°€


word2idx = {w: i for i, w in enumerate(vocab)}
idx2word = {i: w for w, i in word2idx.items()}
vocab_size = len(vocab)



# ì‹œí€€ìŠ¤ ë³€í™˜
def encode(sentence):
    words = sentence.split()  # transform_metadataëŠ” ì´ë¯¸ ì ìš©ë¨
    words = [word if word in word2idx else "<UNK>" for word in words]
    
    return [word2idx.get("<BOS>")] + [word2idx[w] for w in words] + [word2idx.get("<EOS>")]

# ë³€í™˜ëœ book_sentencesì™€ qna_sentencesë¥¼ ì‚¬ìš©í•˜ì—¬ ì¸ì½”ë”© ì§„í–‰
encoded_data = [encode(s) for s in sentences]




BASE_DIR = os.path.dirname(os.path.abspath(__file__))
dict_path  = os.path.join(BASE_DIR, "word2idx.pth")
model_path = os.path.join(BASE_DIR, "chatbot_model.pth")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")



# app/models/ìƒì„±í˜•_basic2_book.py


# â€¦ (ë°ì´í„° ë¡œë“œ, clean_text, extract_question_info ë“± ìœ í‹¸ ì •ì˜) â€¦

# â”€â”€â”€ ëª¨ë¸ ì •ì˜ â”€â”€â”€
class LSTMLanguageModel(nn.Module):
    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256, dropout=0.3):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, embed_dim)
        self.lstm  = nn.LSTM(embed_dim, hidden_dim, batch_first=True)
        self.dropout = nn.Dropout(dropout)
        self.fc    = nn.Linear(hidden_dim, vocab_size)

    def forward(self, x):
        x = self.embed(x)
        out, _ = self.lstm(x)
        out = self.dropout(out)
        return self.fc(out)


# â”€â”€â”€ í•™ìŠµ í•¨ìˆ˜ ì •ì˜ â”€â”€â”€
def train_and_save(
    sequences,
    word2idx,
    vocab_size,
    model_path,
    dict_path,
    batch_size=4,
    max_len=50,
    epochs=1000,
    device='cpu'
):
    # ì‹œí€€ìŠ¤ â†’ Dataset â†’ DataLoader
    class TextDataset(Dataset):
        def __init__(self, seqs):
            self.seqs = [s[:max_len] for s in seqs]
        def __len__(self): return len(self.seqs)
        def __getitem__(self, i):
            s = self.seqs[i]
            x = torch.tensor(s[:-1], dtype=torch.long)
            y = torch.tensor(s[1:],  dtype=torch.long)
            return x, y

    def collate_fn(batch):
        xs, ys = zip(*batch)
        return (
            pad_sequence(xs, batch_first=True, padding_value=word2idx["<PAD>"]),
            pad_sequence(ys, batch_first=True, padding_value=word2idx["<PAD>"])
        )

    dataset   = TextDataset(sequences)
    dataloader= DataLoader(dataset, batch_size=batch_size,
                          shuffle=True, collate_fn=collate_fn)

    # ëª¨ë¸Â·ì˜µí‹°ë§ˆì´ì €Â·ì†ì‹¤í•¨ìˆ˜ ì¤€ë¹„
    model     = LSTMLanguageModel(vocab_size).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    criterion = nn.CrossEntropyLoss()

    # í•™ìŠµ ë£¨í”„
    for epoch in range(1, epochs+1):
        model.train()
        total_loss = 0
        for x, y in dataloader:
            x, y = x.to(device), y.to(device)
            optimizer.zero_grad()
            logits = model(x)
            loss   = criterion(logits.view(-1, vocab_size),
                               y.view(-1))
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f"Epoch {epoch}/{epochs} \tLoss: {total_loss:.4f}")

    # í•™ìŠµ ì™„ë£Œ í›„ ì €ì¥
    torch.save(model.state_dict(), model_path)
    torch.save(word2idx, dict_path)
    print("âœ… ëª¨ë¸ê³¼ ë‹¨ì–´ì‚¬ì „ ì €ì¥ ì™„ë£Œ")




if __name__ == "__main__":

    train_and_save(
        sequences   = encoded_data,  # ìœ„ì—ì„œ ë§Œë“  encoded_data ë¦¬ìŠ¤íŠ¸
        word2idx    = word2idx,
        vocab_size  = vocab_size,
        model_path  = model_path,
        dict_path   = dict_path,
        epochs      = 1000,
        device      = device
    )
    exit()




model = LSTMLanguageModel(vocab_size).to(device)
model.load_state_dict(torch.load(model_path, map_location=device))
model.eval()




def book_info_response(title=None, author=None, genre=None, info_type=None):
    info = search_book_info(title=title, author=author, genre=genre, info_type=info_type)
    if info:
        base = title or author or genre
        return f"ã€{base}ã€ì— ëŒ€í•œ {info_type} ì •ë³´ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: {info}"
    else:
        return "í•´ë‹¹ ì±…ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    
def generate_with_lstm(question, max_len=50):


    prompt = f"ì§ˆë¬¸: {question} ë‹µë³€:"
    input_ids = [word2idx.get("<BOS>")] + [word2idx.get(w, word2idx["<UNK>"]) for w in prompt.split()]
    input_tensor = torch.tensor([input_ids], dtype=torch.long).to(device)

    generated = input_ids.copy()
    for _ in range(max_len):
        with torch.no_grad():
            output = model(input_tensor)
        logits = output[0, -1]

        logits[word2idx["<UNK>"]] = -float('inf')
        logits[word2idx["<PAD>"]] = -float('inf')

        probs = torch.softmax(logits, dim=-1)
        next_token = torch.multinomial(probs, num_samples=1).item()

        generated.append(next_token)

        if next_token == word2idx["<EOS>"]:
            break

        input_tensor = torch.tensor([generated], dtype=torch.long).to(device)

    response_tokens = generated[len(input_ids):generated.index(word2idx["<EOS>"])] \
        if word2idx["<EOS>"] in generated else generated[len(input_ids):]

    response = " ".join([idx2word.get(idx, "") for idx in response_tokens])
    return response.strip()

def answer_question(question):


    # ì±… ì œëª© ë° ìš”ì²­ ì •ë³´ ì¶”ì¶œ + ì±… ì •ë³´ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    title, author, genre, info_type, has_book_info = extract_question_info(question)

    if has_book_info:
        return book_info_response(title, author, genre, info_type)  #  ì±… ì •ë³´ ì‘ë‹µ
    else:
        return generate_with_lstm(question)  #  ì¼ìƒ ëŒ€í™” ì‘ë‹µ


