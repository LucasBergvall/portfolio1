import os
import re
import difflib
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence
from torchvision import models
from sqlalchemy import create_engine
from PIL import Image

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ”§ í™˜ê²½ ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DATABASE_URL = "mysql+pymysql://root:1234@192.168.0.95:13306/db1"
engine = create_engine(DATABASE_URL)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
dict_path = os.path.join(BASE_DIR, "word2idx.pth")
model_path = os.path.join(BASE_DIR, "query_classifier_model.pth")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ“Š ë°ì´í„° ì „ì²˜ë¦¬ ìœ í‹¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def clean_text(text):
    return re.sub(r"[^ê°€-í£a-zA-Z0-9\s.,!?~]", "", str(text))

def remove_particles(sentence):
    particles = ["ì´", "ê°€", "ì€", "ëŠ”", "ì„", "ë¥¼", "ì—", "ì—ì„œ",
                 "ì—ê²Œ", "ê»˜", "ë„", "ë§Œ", "ê³¼", "ì™€", "ë¡œ", "ìœ¼ë¡œ",
                 "ë¶€í„°", "ê¹Œì§€", "ì˜", "ì—ê²Œì„œ"]
    for p in particles:
        sentence = re.sub(rf"\b{p}\b", "", sentence)
    return sentence.strip()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CSV(QnA)ì™€ DB(ì±… ì •ë³´)ì—ì„œ í•™ìŠµ ìƒ˜í”Œ êµ¬ì„±
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CSVì˜ QnA ë°ì´í„° ë¡œë“œ (ê²½ë¡œëŠ” í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •)
qa_csv_path = os.path.join(BASE_DIR, "data", "bookstore_chatbot_qa1_utf8.csv")
qa_df = pd.read_csv(qa_csv_path)

# DBì˜ ì±… ì •ë³´ë¥¼ ë¡œë“œ (ì±… ì œëª©ì„ ì¿¼ë¦¬ë¡œ ì‚¬ìš©)
def load_book_info():
    df = pd.read_sql("SELECT * FROM book_info", engine)
    df.rename(columns={
        "book_name_info": "íƒ€ì´í‹€",
        "writer_info": "ì‘ê°€",
        "book_detail_info": "ë‚´ìš©",
        "publisher_info": "ì¶œíŒì‚¬",
        "price_info": "ì›ë³¸ ê°€ê²©",
        "genre_info": "ì¹´í…Œê³ ë¦¬",
        "book_img_info": "ì´ë¯¸ì§€ íŒŒì¼"
    }, inplace=True)
    return df

book_df = load_book_info()

# QnA ìƒ˜í”Œ: CSVì˜ 'ì§ˆë¬¸' í…ìŠ¤íŠ¸ë¥¼ ì „ì²˜ë¦¬í•œ í›„ ì‚¬ìš© (í´ë˜ìŠ¤ 0)
qna_samples = [remove_particles(clean_text(q)) for q in qa_df['ì§ˆë¬¸'].tolist()]

# Book ìƒ˜í”Œ: DBì˜ ì±… ì •ë³´ì—ì„œ 'íƒ€ì´í‹€' ë˜ëŠ” "ì œëª©"ì— í•´ë‹¹í•˜ëŠ” í…ìŠ¤íŠ¸ (í´ë˜ìŠ¤ 1)
book_samples = [remove_particles(clean_text(title)) for title in book_df['íƒ€ì´í‹€'].tolist()]

# ê°ê°ì˜ ìƒ˜í”Œì— ëŒ€í•´ ë¼ë²¨ ì§€ì •
# QnA â†’ 0 , Book â†’ 1
all_samples = qna_samples + book_samples
labels = [0] * len(qna_samples) + [1] * len(book_samples)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ”¤ í† í¬ë‚˜ì´ì € & ë‹¨ì–´ ì‚¬ì „ ìƒì„± (ëª¨ë“  ì…ë ¥ ë¬¸ì¥ì„ ëŒ€ìƒìœ¼ë¡œ)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def tokenize(sentence):
    return sentence.split()

def build_vocab(sentences):
    tokens = [word for s in sentences for word in tokenize(s)]
    base_vocab = ["<PAD>", "<BOS>", "<EOS>", "<UNK>"]
    vocab = base_vocab + list(set(tokens))
    word2idx = {w: i for i, w in enumerate(vocab)}
    idx2word = {i: w for w, i in word2idx.items()}
    return word2idx, idx2word, len(vocab)

word2idx, idx2word, vocab_size = build_vocab(all_samples)

def encode(sentence, word2idx):
    words = tokenize(sentence)
    words = [w if w in word2idx else "<UNK>" for w in words]
    return [word2idx["<BOS>"]] + [word2idx[w] for w in words] + [word2idx["<EOS>"]]

encoded_samples = [encode(s, word2idx) for s in all_samples]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ§  ë¶„ë¥˜ ëª¨ë¸ ì •ì˜ (QueryClassifier)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class QueryClassifier(nn.Module):
    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256, num_classes=2):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, embed_dim)
        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, num_classes)
    
    def forward(self, x):
        x = self.embed(x)
        _, (h, _) = self.lstm(x)
        # h[-1] : ë§ˆì§€ë§‰ layerì˜ hidden state
        return self.fc(h[-1])

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ‹ï¸â€â™‚ï¸ í•™ìŠµ ë°ì´í„°ì…‹ ë° DataLoader (ë¶„ë¥˜ìš©)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class QueryDataset(Dataset):
    def __init__(self, sequences, labels):
        self.sequences = sequences
        self.labels = labels
    def __len__(self):
        return len(self.sequences)
    def __getitem__(self, index):
        return torch.tensor(self.sequences[index]), torch.tensor(self.labels[index])

def collate_fn(batch):
    xs, ys = zip(*batch)
    xs = pad_sequence(xs, batch_first=True, padding_value=word2idx["<PAD>"])
    ys = torch.stack(ys)
    return xs.to(device), ys.to(device)

dataset = QueryDataset(encoded_samples, labels)
dataloader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ‹ï¸â€â™‚ï¸ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ë° ì €ì¥
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def train_and_save_classifier(model, dataloader, vocab_size, model_path, epochs=10, device='cpu'):
    model.to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    criterion = nn.CrossEntropyLoss()
    
    for epoch in range(1, epochs+1):
        model.train()
        total_loss = 0.0
        for x, y in dataloader:
            optimizer.zero_grad()
            logits = model(x)
            loss = criterion(logits, y)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f"[Epoch {epoch}] Loss: {total_loss:.4f}")
    
    torch.save(model.state_dict(), model_path)
    torch.save(word2idx, dict_path)
    print("âœ… ë¶„ë¥˜ ëª¨ë¸ ë° ë‹¨ì–´ ì‚¬ì „ ì €ì¥ ì™„ë£Œ")

classifier_model = QueryClassifier(vocab_size)
train_and_save_classifier(classifier_model, dataloader, vocab_size, model_path, epochs=100, device=device)